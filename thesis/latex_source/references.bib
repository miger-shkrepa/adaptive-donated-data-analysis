@misc{Rao_2016,
        author={Rao, Adithya and Spasojevic, Nemanja and Li, Zhisheng and Dsouza, Trevor},
	title={Klout score: Measuring influence across multiple social networks},
	url = {https://www.slideshare.net/slideshow/klout-score-measuring-influence-across-multiple-social-networks/57298856},
	language = {en},
	urldate = {2025-04-15},
	journal = {SlideShare},
	month = jan,
	year = {2016},
}

@article{IJoC8650,
	author = {Andreas Hepp and Andreas Breiter and Thomas Friemel},
	title = { Digital Traces in Context| Digital Traces in Context — An Introduction },
	journal = {International Journal of Communication},
	volume = {12},
	number = {0},
	year = {2018},
	keywords = {digital traces, contextualization, big data, digital methods, datafication, deep mediatization},
	abstract = {A consequence of living in a media-saturated world is that we inevitably leave behind digital traces of our media use. In this introduction to the International Journal of Communication’s thematic section, we argue for a need to put those digital traces in context. As a starting point, we outline our basic understanding of digital traces, generally defining them as numerically produced correlations of disparate kinds of data that are generated by our practices in a media environment characterized by digitalization. On this basis, we distinguish three contextual facets that are of relevance when considering digital traces: first, the context of the scientific discourse in which research on digital traces is positioned; second, the context of the methods being applied to researching them; and third, the aforementioned context of the empirical field. With reference to the articles in this thematic section, this introduction argues that, in a single study, all three contextual facets interact as the scientific discourse relates to the methods being used, which in turn relates to the entire field of research.},
	issn = {1932-8036},	url = {https://ijoc.org/index.php/ijoc/article/view/8650}
}

@article{IndiraSen2021ApplyingAT,
  title={Applying a total error framework for digital traces to social media research},
  author={Indira Sen and Fabian Flöck and Katrin Weller and Bernd Weiss and Claudia Wagner},
  journal={Handbook of Computational Social Science, Volume 2},
  year={2021},
  volume={1 1},
  doi={10.4324/9781003025245-11}
}

@article{doi:10.1177/2057047316683200,
author = {Veronica Barassi},
title ={Datafied Citizens? Social Media Activism, Digital Traces and the Question about Political Profiling},
journal = {Communication and the Public},
volume = {1},
number = {4},
pages = {494-499},
year = {2016},
doi = {10.1177/2057047316683200},
URL = {https://doi.org/10.1177/2057047316683200}
}

@article{annurev:/content/journals/10.1146/annurev-soc-060116-053457,
   author = "Lazer, David and Radford, Jason",
   title = "Data ex Machina: Introduction to Big Data", 
   journal= "Annual Review of Sociology",
   year = "2017",
   volume = "43",
   number = "Volume 43, 2017",
   pages = "19-39",
   doi = "https://doi.org/10.1146/annurev-soc-060116-053457",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-soc-060116-053457",
   publisher = "Annual Reviews",
   issn = "1545-2115",
   type = "Journal Article",
   keywords = "CDR data",
   keywords = "networks",
   keywords = "computational social science",
   keywords = "research ethics",
   keywords = "big data",
   keywords = "technology",
   keywords = "quantitative sociology",
   keywords = "mobility",
   keywords = "research design, social media",
   keywords = "data linkage",
   keywords = "research methodology",
   abstract = "Social life increasingly occurs in digital environments and continues to be mediated by digital systems. Big data represents the data being generated by the digitization of social life, which we break down into three domains: digital life, digital traces, and digitalized life. We argue that there is enormous potential in using big data to study a variety of phenomena that remain difficult to observe. However, there are some recurring vulnerabilities that should be addressed. We also outline the role institutions must play in clarifying the ethical rules of the road. Finally, we conclude by pointing to a number of nascent but important trends in the use of big data.",
  }

 @article{Cesare_Lee_McCormick_Spiro_Zagheni_2018, title={Promises and Pitfalls of Using Digital Traces for Demographic Research}, volume={55}, ISSN={0070-3370, 1533-7790}, DOI={10.1007/s13524-018-0715-2}, abstractNote={The digital traces that we leave online are increasingly fruitful sources of data for social scientists, including those interested in demographic research. The collection and use of digital data also presents numerous statistical, computational, and ethical challenges, motivating the development of new research approaches to address these burgeoning issues. In this article, we argue that researchers with formal training in demography—those who have a history of developing innovative approaches to using challenging data—are well positioned to contribute to this area of work. We discuss the benefits and challenges of using digital trace data for social and demographic research, and we review examples of current demographic literature that creatively use digital trace data to study processes related to fertility, mortality, and migration. Focusing on Facebook data for advertisers—a novel “digital census” that has largely been untapped by demographers—we provide illustrative and empirical examples of how demographic researchers can manage issues such as bias and representation when using digital trace data. We conclude by offering our perspective on the road ahead regarding demography and its role in the data revolution.}, number={5}, journal={Demography}, author={Cesare, Nina and Lee, Hedwig and McCormick, Tyler and Spiro, Emma and Zagheni, Emilio}, year={2018}, month=oct, pages={1979–1999}, language={en} }

 @inbook{Keusch_Kreuter_2021, address={London}, edition={1}, title={Digital trace data}, ISBN={978-1-003-02458-3}, url={https://www.taylorfrancis.com/books/9781003024583/chapters/10.4324/9781003024583-8}, DOI={10.4324/9781003024583-8}, booktitle={Handbook of Computational Social Science, Volume 1}, publisher={Routledge}, author={Keusch, Florian and Kreuter, Frauke}, year={2021}, month=nov, pages={100–118}, language={en} }

@article{
doi:10.1161/CIRCOUTCOMES.123.010150,
author = {Adrian F. Hernandez },
title = {Everything, Everywhere All at Once: Evidence Generation and Implementation in the Digital Age},
journal = {Circulation: Cardiovascular Quality and Outcomes},
volume = {17},
number = {6},
pages = {e010150},
year = {2024},
doi = {10.1161/CIRCOUTCOMES.123.010150},
URL = {https://www.ahajournals.org/doi/abs/10.1161/CIRCOUTCOMES.123.010150},
eprint = {https://www.ahajournals.org/doi/pdf/10.1161/CIRCOUTCOMES.123.010150}}

@article{doi:10.1177/20539517231213827,
author = {Andrea Armstrong and Jo Briggs and Wendy Moncur and 
Daniel Paul Carey and Emma Nicol and Burkhard Schafer},
title ={Everyday digital traces},
journal = {Big Data \& Society},
volume = {10},
number = {2},
pages = {20539517231213827},
year = {2023},
doi = {10.1177/20539517231213827},
URL = {https://doi.org/10.1177/20539517231213827},
eprint = {https://doi.org/10.1177/20539517231213827},
abstract = { Our research responds to calls for more engagement with everyday personal data. We used a co-designed, fictional persona called Alex Smith to concretise and represent people's online information to help participants (through role-playing) reflect on data and digital traces. Drawing together four fields of scholarly research concerning personal data: digital traces and the digital self, datafication and dataveillance, mundane, everyday data and the data journey – our aim was to advance understandings of personal data by exploring ordinary people's seemingly innocuous digital traces generated through everyday online interactions. Our paper presents three key findings from our analysis: (1) how ordinary people cope with and manage everyday data; (2) the haunting effects and affects of peer-to-peer surveillance and (3) postdigital identities. We argue that greater attention needs to be paid to everyday digital traces – how they are understood, managed and revealed because this has implications for ordinary people, corporate entities and governments. We contribute to a gap in critical data studies literature that calls for further investigations into ordinary people's engagement with data. We also offer a method that can be adapted for and used with different participant groups, which also supports their awareness of cumulative functions of personal data and potential use by un/known actors. }
}

@article{doi:10.1177/0963721419861410,
author = {Anat Rafaeli and Shelly Ashtar and Daniel Altman},
title ={Digital Traces: New Data, Resources, and Tools for Psychological-Science Research},
journal = {Current Directions in Psychological Science},
volume = {28},
number = {6},
pages = {560-566},
year = {2019},
doi = {10.1177/0963721419861410},
URL = {https://doi.org/10.1177/0963721419861410},
eprint = {https://doi.org/10.1177/0963721419861410},
abstract = { New technologies create and archive digital traces—records of people’s behavior—that can supplement and enrich psychological research. Digital traces offer psychological-science researchers novel, large-scale data (which reflect people’s actual behaviors), rapidly collected and analyzed by new tools. We promote the integration of digital-traces data into psychological science, suggesting that it can enrich and overcome limitations of current research. In this article, we review helpful data sources, tools, and resources and discuss challenges associated with using digital traces in psychological research. Our review positions digital-traces research as complementary to traditional psychological-research methods and as offering the potential to enrich insights on human psychology. }
}

@article{Boeschoten2023, doi = {10.21105/joss.05596}, url = {https://doi.org/10.21105/joss.05596}, year = {2023}, publisher = {The Open Journal}, volume = {8}, number = {90}, pages = {5596}, author = {Laura Boeschoten and Niek C. de Schipper and Adriënne M. Mendrik and Emiel van der Veen and Bella Struminskaya and Heleen Janssen and Theo Araujo}, title = {Port: A software tool for digital data donation}, journal = {Journal of Open Source Software} }

@Legislation{europeanParliament2016a,
  date       = {2016-05-04},
  location   = {OJ L 119, 4.5.2016, p. 1--88},
  title      = {Regulation ({EU}) 2016/679 of the {European} {Parliament} and of the {Council}},
  url        = {https://data.europa.eu/eli/reg/2016/679/oj},
  titleaddon = {of 27 {April} 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing {Directive} 95/46/{EC} ({General} {Data} {Protection} {Regulation})},
  abstract   = {The General Data Protection Regulation (2016/679, "GDPR") is a Regulation in European Union (EU) law on data protection and privacy in the EU and the European Economic Area (EEA).},
  author     = {{European Parliament} and {Council of the European Union}},
  keywords   = {access consumer data data-processing freedom gdpr information justice law personal privacy protection security verification},
  urldate    = {2023-04-13},
}

@article{boeschoten2021automatic,
  title={Automatic de-identification of data download packages},
  author={Boeschoten, Laura and Voorvaart, Roos and Van Den Goorbergh, Ruben and Kaandorp, Casper and De Vos, Martine},
  journal={Data Science},
  volume={4},
  number={2},
  pages={101--120},
  year={2021},
  publisher={IOS Press}
}

@article{Ohme_Araujo_Boeschoten_Freelon_Ram_Reeves_Robinson_2024, title={Digital Trace Data Collection for Social Media Effects Research: APIs, Data Donation, and (Screen) Tracking}, volume={18}, ISSN={1931-2458, 1931-2466}, DOI={10.1080/19312458.2023.2181319}, number={2}, journal={Communication Methods and Measures}, author={Ohme, Jakob and Araujo, Theo and Boeschoten, Laura and Freelon, Deen and Ram, Nilam and Reeves, Byron B. and Robinson, Thomas N.}, year={2024}, month=apr, pages={124–141}, language={en} }

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@misc{naveed2024comprehensiveoverviewlargelanguage,
      title={A Comprehensive Overview of Large Language Models}, 
      author={Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
      year={2024},
      eprint={2307.06435},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.06435}, 
}

@phdthesis{lau1994adaptive,
  title={Adaptive statistical language modeling},
  author={Lau, Raymond},
  year={1994},
  school={Massachusetts Institute of Technology}
}

@misc{wang2024historydevelopmentprincipleslarge,
      title={History, Development, and Principles of Large Language Models-An Introductory Survey}, 
      author={Zichong Wang and Zhibo Chu and Thang Viet Doan and Shiwen Ni and Min Yang and Wenbin Zhang},
      year={2024},
      eprint={2402.06853},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06853}, 
}

@article{10.1145/3490443,
author = {Li, Hang},
title = {Language models: past, present, and future},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3490443},
doi = {10.1145/3490443},
abstract = {A language modeling overview, highlighting basic concepts, intuitive explanations, technical achievements, and fundamental challenges.},
journal = {Commun. ACM},
month = jun,
pages = {56–63},
numpages = {8}
}

@article{review10433480,
  author={Raiaan, Mohaimenul Azam Khan and Mukta, Md. Saddam Hossain and Fatema, Kaniz and Fahad, Nur Mohammad and Sakib, Sadman and Mim, Most Marufatul Jannat and Ahmad, Jubaer and Ali, Mohammed Eunus and Azam, Sami},
  journal={IEEE Access}, 
  title={A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges}, 
  year={2024},
  volume={12},
  number={},
  pages={26839-26874},
  keywords={Cognition;Artificial intelligence;Transformers;Training;Taxonomy;Task analysis;Surveys;Natural language processing;Question answering (information retrieval);Information analysis;Linguistics;Large language models (LLM);natural language processing (NLP);artificial intelligence;transformer;pre-trained models;taxonomy;application},
  doi={10.1109/ACCESS.2024.3365742}}

@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal={Journal of machine learning research},
  volume={3},
  number={Feb},
  pages={1137--1155},
  year={2003}
}

@misc{mikolov2013efficientestimationwordrepresentations,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}

@article{10.1162/neco.1997.9.8.1735,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}

@article{WANG202351,
    title = {Pre-Trained Language Models and Their Applications},
    journal = {Engineering},
    volume = {25},
    pages = {51-65},
    year = {2023},
    issn = {2095-8099},
    doi = {https://doi.org/10.1016/j.eng.2022.04.024},
    url = {https://www.sciencedirect.com/science/article/pii/S2095809922006324},
    author = {Haifeng Wang and Jiwei Li and Hua Wu and Eduard Hovy and Yu Sun},
    keywords = {Pre-trained models, Natural language processing},
    abstract = {Pre-trained language models have achieved striking success in natural language processing (NLP), leading to a paradigm shift from supervised learning to pre-training followed by fine-tuning. The NLP community has witnessed a surge of research interest in improving pre-trained models. This article presents a comprehensive review of representative work and recent progress in the NLP field and introduces the taxonomy of pre-trained models. We first give a brief introduction of pre-trained models, followed by characteristic methods and frameworks. We then introduce and analyze the impact and challenges of pre-trained models and their downstream applications. Finally, we briefly conclude and address future research directions in this field.}
}

@article{10.1145/3649449,
    author = {Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
    title = {Pre-Trained Language Models for Text Generation: A Survey},
    year = {2024},
    issue_date = {September 2024},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {56},
    number = {9},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3649449},
    doi = {10.1145/3649449},
    abstract = {Text Generation aims to produce plausible and readable text in human language from input data. The resurgence of deep learning has greatly advanced this field, in particular, with the help of neural generation models based on pre-trained language models (PLMs). Text generation based on PLMs is viewed as a promising approach in both academia and industry. In this article, we provide a survey on the utilization of PLMs in text generation. We begin with introducing two key aspects of applying PLMs to text generation: (1) how to design an effective PLM to serve as the generation model; and (2) how to effectively optimize PLMs given the reference text and to ensure that the generated texts satisfy special text properties. Then, we show the major challenges that have arisen in these aspects, as well as possible solutions for them. We also include a summary of various useful resources and typical text generation applications based on PLMs. Finally, we highlight the future research directions which will further improve these PLMs for text generation. This comprehensive survey is intended to help researchers interested in text generation problems to learn the core concepts, the main techniques and the latest developments in this area based on PLMs.},
    journal = {ACM Comput. Surv.},
    month = apr,
    articleno = {230},
    numpages = {39},
    keywords = {Pre-trained language models, natural language processing}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{singh2024tokenizationcountsimpacttokenization,
      title={Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs}, 
      author={Aaditya K. Singh and DJ Strouse},
      year={2024},
      eprint={2402.14903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14903}, 
}

@misc{schmidt2024tokenizationcompression,
      title={Tokenization Is More Than Compression}, 
      author={Craig W. Schmidt and Varshini Reddy and Haoran Zhang and Alec Alameddine and Omri Uzan and Yuval Pinter and Chris Tanner},
      year={2024},
      eprint={2402.18376},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.18376}, 
}

@misc{guan2025attentionmechanismsperspectiveexploring,
      title={Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data}, 
      author={Zhong Guan and Likang Wu and Hongke Zhao and Ming He and Jianpin Fan},
      year={2025},
      eprint={2505.02130},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.02130}, 
}

@INPROCEEDINGS{11027973,
  author={Sun, Siyuan and Yu, Jinling and Liu, Hanshuo and Guo, Hanyun and Cao, Yang and Zhang, Shouhua and Zhou, Jiehan},
  booktitle={2025 8th World Conference on Computing and Communication Technologies (WCCCT)}, 
  title={Optimizing Attention for Efficient LLM Inference: A Review}, 
  year={2025},
  volume={},
  number={},
  pages={482-491},
  keywords={Attention mechanisms;Quantization (signal);Reviews;Computational modeling;Large language models;Memory management;Optimization methods;Real-time systems;Computational efficiency;Computational complexity;Large Language Models;Attention Mechanism Optimization;Inference Efficiency;Long-Sequence Modeling},
  doi={10.1109/WCCCT65447.2025.11027973}
}

@article{NIU202148,
    title = {A review on the attention mechanism of deep learning},
    journal = {Neurocomputing},
    volume = {452},
    pages = {48-62},
    year = {2021},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/j.neucom.2021.03.091},
    url = {https://www.sciencedirect.com/science/article/pii/S092523122100477X},
    author = {Zhaoyang Niu and Guoqiang Zhong and Hui Yu},
    keywords = {Attention mechanism, Deep learning, Recurrent Neural Network (RNN), Convolutional Neural Network (CNN), Encoder-decoder, Unified attention model, Computer vision applications, Natural language processing applications},
    abstract = {Attention has arguably become one of the most important concepts in the deep learning field. It is inspired by the biological systems of humans that tend to focus on the distinctive parts when processing large amounts of information. With the development of deep neural networks, attention mechanism has been widely used in diverse application domains. This paper aims to give an overview of the state-of-the-art attention models proposed in recent years. Toward a better general understanding of attention mechanisms, we define a unified model that is suitable for most attention structures. Each step of the attention mechanism implemented in the model is described in detail. Furthermore, we classify existing attention models according to four criteria: the softness of attention, forms of input feature, input representation, and output representation. Besides, we summarize network architectures used in conjunction with the attention mechanism and describe some typical applications of attention mechanism. Finally, we discuss the interpretability that attention brings to deep learning and present its potential future trends.}
}

@phdthesis{ignijic2024evaluating,
  title={Evaluating Adaptive Activation Functions in Language Models},
  author={Ignijic, Filip},
  year={2024},
  school={Delft University of Technology}
}

@article{activationfunction,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

@inproceedings{10.1145/3626772.3657807,
    author = {Lin, Xinyu and Wang, Wenjie and Li, Yongqi and Yang, Shuo and Feng, Fuli and Wei, Yinwei and Chua, Tat-Seng},
    title = {Data-efficient Fine-tuning for LLM-based Recommendation},
    year = {2024},
    isbn = {9798400704314},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3626772.3657807},
    doi = {10.1145/3626772.3657807},
    abstract = {Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data. To tackle these issues, we introduce two primary objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process. To pursue the two objectives, we propose a novel data pruning method incorporating two scores, namely influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of removing each sample on the overall performance. To achieve low costs of the data pruning process, we employ a small-sized surrogate model to replace LLMs to obtain the influence score. Considering the potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs. We instantiate the proposed method on two competitive LLM-based recommender models, and empirical results on three real-world datasets validate the effectiveness of our proposed method. In particular, our method uses only 2\% samples to surpass the full data fine-tuning, reducing time costs by 97\%.},
    booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
    pages = {365–374},
    numpages = {10},
    keywords = {data pruning, efficient fine-tuning, llm-based recommendation},
    location = {Washington DC, USA},
    series = {SIGIR '24}
}

@Article{bdcc9040087,
    AUTHOR = {Wu, Xiao-Kun and Chen, Min and Li, Wanyi and Wang, Rui and Lu, Limeng and Liu, Jia and Hwang, Kai and Hao, Yixue and Pan, Yanru and Meng, Qingguo and Huang, Kaibin and Hu, Long and Guizani, Mohsen and Chao, Naipeng and Fortino, Giancarlo and Lin, Fei and Tian, Yonglin and Niyato, Dusit and Wang, Fei-Yue},
    TITLE = {LLM Fine-Tuning: Concepts, Opportunities, and Challenges},
    JOURNAL = {Big Data and Cognitive Computing},
    VOLUME = {9},
    YEAR = {2025},
    NUMBER = {4},
    ARTICLE-NUMBER = {87},
    URL = {https://www.mdpi.com/2504-2289/9/4/87},
    ISSN = {2504-2289},
    ABSTRACT = {As a foundation of large language models, fine-tuning drives rapid progress, broad applicability, and profound impacts on human–AI collaboration, surpassing earlier technological advancements. This paper provides a comprehensive overview of large language model (LLM) fine-tuning by integrating hermeneutic theories of human comprehension, with a focus on the essential cognitive conditions that underpin this process. Drawing on Gadamer’s concepts of Vorverständnis, Distanciation, and the Hermeneutic Circle, the paper explores how LLM fine-tuning evolves from initial learning to deeper comprehension, ultimately advancing toward self-awareness. It examines the core principles, development, and applications of fine-tuning techniques, emphasizing its growing significance across diverse field and industries. The paper introduces a new term, “Tutorial Fine-Tuning (TFT)”, which annotates a process of intensive tuition given by a “tutor” to a small number of “students”, to define the latest round of LLM fine-tuning advancements. By addressing key challenges associated with fine-tuning, including ensuring adaptability, precision, credibility and reliability, this paper explores potential future directions for the co-evolution of humans and AI. By bridging theoretical perspectives with practical implications, this work provides valuable insights into the ongoing development of LLMs, emphasizing their potential to achieve higher levels of cognitive and operational intelligence.},
    DOI = {10.3390/bdcc9040087}
}

@InProceedings{10.1007/978-981-99-7962-2_30,
    author="Marvin, Ggaliwango
    and Hellen, Nakayiza
    and Jjingo, Daudi
    and Nakatumba-Nabende, Joyce",
    editor="Jacob, I. Jeena
    and Piramuthu, Selwyn
    and Falkowski-Gilski, Przemyslaw",
    title="Prompt Engineering in Large Language Models",
    booktitle="Data Intelligence and Cognitive Informatics",
    year="2024",
    publisher="Springer Nature Singapore",
    address="Singapore",
    pages="387--402",
    abstract="With the undeniable rapid development of Conversational Artificial Intelligence (AI) particularly Large Language Models (LLMs), prompt engineering has become an obligatory skill for effective communication and interaction with language driven tools like ChatGPT. It can be leveraged in enforcing rules and process automation for ensuring good quality and quantity of output from LLMs. Moreover, the order of providing examples within prompts, automatic instruction generation, and selection methods has been proven to significantly impact the performance of LLMs. Prompts can be optimized to maximize a chosen score function by searching a pool of instruction candidates within LLMs. No wonder automatically generated instructions give better or similar performance than human annotated instructions and outperform baselines of LLMs, this makes prompt engineering a programming procedure for customizing outputs and interactions of LLMs. In this chapter, we provide thorough understanding of prompt engineering, latest prompt engineering techniques with relevant exercises for putting the techniques in practice. We also discuss current and future trends of LLMs and prompt engineering research, including the rise of automatic instruction generation and selection methods. These are very important for prompt and NLP engineers, conversational AI researchers, and all information seekers or users of LLMs and prompt engineering tools in sensitive domains like health care, security, education among others. The chapter provides indepth understanding of prompt engineering principles and techniques for responsible coversational AI.",
    isbn="978-981-99-7962-2"
}

@misc{bhandari2024surveypromptingtechniquesllms,
      title={A Survey on Prompting Techniques in LLMs}, 
      author={Prabin Bhandari},
      year={2024},
      eprint={2312.03740},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.03740}, 
}

@misc{chew2023llmassistedcontentanalysisusing,
      title={LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding}, 
      author={Robert Chew and John Bollenbacher and Michael Wenger and Jessica Speer and Annice Kim},
      year={2023},
      eprint={2306.14924},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.14924}, 
}

@phdthesis{simon2023promptguidelines,
  title        = {Prompt Engineering Guidelines for LLMs in Requirements Engineering},
  author       = {Simon Arvidsson and Johan Axell},
  year         = {2023},
  month        = jun,
  address      = {Gothenburg, Sweden},
  note         = {Available at \url{https://hdl.handle.net/2077/77967}},
  school       = {University of Gothenburg, Chalmers University of Technology},
  type         = {Bachelor's thesis}
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10997}, 
}

@misc{liu2024raghelpreasoningllm,
      title={How Much Can RAG Help the Reasoning of LLM?}, 
      author={Jingyu Liu and Jiaen Lin and Yong Liu},
      year={2024},
      eprint={2410.02338},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.02338}, 
}

@misc{bartczak2024rag,
  title={From RAG to Riches: Evaluating the Benefits of Retrieval-Augmented Generation in SQL Database Querying},
  author={Bartczak, Zuzanna},
  year={2024},
  url={https://www.diva-portal.org/smash/get/diva2:1905130/FULLTEXT01.pdf}
}

@misc{wang2024evaluatingqualityanswersretrievalaugmented,
      title={Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need}, 
      author={Yang Wang and Alberto Garcia Hernandez and Roman Kyslyi and Nicholas Kersting},
      year={2024},
      eprint={2406.18064},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18064}, 
}

@article{10.14778/3685800.3685905,
    author = {Zhao, Xinyang and Zhou, Xuanhe and Li, Guoliang},
    title = {Chat2Data: An Interactive Data Analysis System with RAG, Vector Databases and LLMs},
    year = {2024},
    issue_date = {August 2024},
    publisher = {VLDB Endowment},
    volume = {17},
    number = {12},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/3685800.3685905},
    doi = {10.14778/3685800.3685905},
    abstract = {Traditional data analysis methods require users to write programming codes or issue SQL queries to analyze the data, which are inconvenient for ordinary users. Large language models (LLMs) can alleviate these limitations by enabling users to interact with the data with natural language (NL), e.g., result retrieval and summarization for unstructured data and transforming the NL text to SQL queries or codes for structured data. However, existing LLMs have three limitations: hallucination (due to lacking domain knowledge for vertical domains), high cost for LLM reasoning, and low accuracy for complicated tasks. To address these problems, we propose a prototype, Chat2Data, to interactively analyze the data with natural language. Chat2Data adopts a three-layer method, where the first layer uses Retrieval-Augmented Generation (RAG) to embed domain knowledge in order to address the hallucination problem, the second layer utilizes vector databases to reduce the number of interactions with LLMs so as to improve the performance, and the third layer designs a pipeline agent to decompose a complex task to multiple subtasks and use multiple round reasoning to generate the results in order to improve the accuracy of LLMs. We demonstrate Chat2Data with two real scenarios, unstructured data retrieval and summarization, and natural language-based structured data analysis. The online demo is available at http://vdemo.dbmind.cn.},
    journal = {Proc. VLDB Endow.},
    month = aug,
    pages = {4481–4484},
    numpages = {4}
}

@misc{wang2024bioragragllmframeworkbiological,
      title={BioRAG: A RAG-LLM Framework for Biological Question Reasoning}, 
      author={Chengrui Wang and Qingqing Long and Meng Xiao and Xunxin Cai and Chengjun Wu and Zhen Meng and Xuezhi Wang and Yuanchun Zhou},
      year={2024},
      eprint={2408.01107},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.01107}, 
}

@misc{zhao2024retrievalaugmentedgenerationaigeneratedcontent,
      title={Retrieval-Augmented Generation for AI-Generated Content: A Survey}, 
      author={Penghao Zhao and Hailin Zhang and Qinhan Yu and Zhengren Wang and Yunteng Geng and Fangcheng Fu and Ling Yang and Wentao Zhang and Jie Jiang and Bin Cui},
      year={2024},
      eprint={2402.19473},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.19473}, 
}

@inbook{Yu_2025,
   title={Evaluation of Retrieval-Augmented Generation: A Survey},
   isbn={9789819610242},
   ISSN={1865-0937},
   url={http://dx.doi.org/10.1007/978-981-96-1024-2_8},
   DOI={10.1007/978-981-96-1024-2_8},
   booktitle={Big Data},
   publisher={Springer Nature Singapore},
   author={Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
   year={2025},
   pages={102–120} 
}

@inproceedings{10.1145/3626772.3657957,
    author = {Salemi, Alireza and Zamani, Hamed},
    title = {Evaluating Retrieval Quality in Retrieval-Augmented Generation},
    year = {2024},
    isbn = {9798400704314},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3626772.3657957},
    doi = {10.1145/3626772.3657957},
    abstract = {Evaluating retrieval-augmented generation (RAG) presents challenges, particularly for retrieval models within these systems. Traditional end-to-end evaluation methods are computationally expensive. Furthermore, evaluation of the retrieval model's performance based on query-document relevance labels shows a small correlation with the RAG system's downstream performance. We propose a novel evaluation approach, eRAG, where each document in the retrieval list is individually utilized by the large language model within the RAG system. The output generated for each document is then evaluated based on the downstream task ground truth labels. In this manner, the downstream performance for each document serves as its relevance label. We employ various downstream task metrics to obtain document-level annotations and aggregate them using set-based or ranking metrics. Extensive experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall's tau correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.},
    booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
    pages = {2395–2400},
    numpages = {6},
    keywords = {evaluation, retrieval quality, retrieval-augmented generation},
    location = {Washington DC, USA},
    series = {SIGIR '24}
}

@article{10.1145/3624724,
    author = {Shanahan, Murray},
    title = {Talking about Large Language Models},
    year = {2024},
    issue_date = {February 2024},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {67},
    number = {2},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3624724},
    doi = {10.1145/3624724},
    abstract = {Interacting with a contemporary LLM-based conversational agent can create an illusion of being in the presence of a thinking creature. Yet, in their very nature, such systems are fundamentally not like us.},
    journal = {Commun. ACM},
    month = jan,
    pages = {68–79},
    numpages = {12}
}

@misc{ma2023demonstrationinsightpilotllmempoweredautomated,
      title={Demonstration of InsightPilot: An LLM-Empowered Automated Data Exploration System}, 
      author={Pingchuan Ma and Rui Ding and Shuai Wang and Shi Han and Dongmei Zhang},
      year={2023},
      eprint={2304.00477},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2304.00477}, 
}

@misc{openai_gpt35_turbo,
    author = {OpenAI},
    title = {GPT-3.5 Turbo Model},
    year = {},
    howpublished = {\url{https://platform.openai.com/docs/models/gpt-3.5-turbo}},
    note = {Online; accessed 2025-07-06}
}

@misc{openai_code_interpreter_tool,
  author = {OpenAI},
  title = {Assistants Code Interpreter},
  year = {},
  howpublished = {\url{https://platform.openai.com/docs/assistants/tools/code-interpreter}},
  note = {Online; accessed 2025-07-06}
}

@misc{langchain_pandas,
  author = {LangChain},
  title = {LangChain Pandas Dataframe Agent},
  howpublished = {\url{https://python.langchain.com/docs/integrations/tools/pandas/}},
  year = {2023},
  note = "[Online; accessed 2025-07-06]"
}

@misc{openai_embedding_ada,
  author = {OpenAI},
  title = {text-embedding-ada-002},
  howpublished = {\url{https://platform.openai.com/docs/models/text-embedding-ada-002}},
  month = dec,
  year = {2022},
  note = "[Online; accessed 2025-07-06]"
}

@misc{liu2023jarvixllmcodeplatform,
      title={JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization}, 
      author={Shang-Ching Liu and ShengKun Wang and Wenqi Lin and Chung-Wei Hsiung and Yi-Chen Hsieh and Yu-Ping Cheng and Sian-Hong Luo and Tsungyao Chang and Jianwei Zhang},
      year={2023},
      eprint={2312.02213},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.02213}, 
}

@misc{vicuna-13B-1.1-GPTQ-4bit-128g,
  title = {vicuna-13B-1.1-GPTQ-4bit-128g Model},
  howpublished = {\url{https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g?doi=true}},
  author = {TheBloke},
  year = {2023},
  note = "[Online; accessed 2025-07-06]"
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{wang2025financialanalysisintelligentfinancial,
      title={Financial Analysis: Intelligent Financial Data Analysis System Based on LLM-RAG}, 
      author={Jingru Wang and Wen Ding and Xiaotong Zhu},
      year={2025},
      eprint={2504.06279},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST},
      url={https://arxiv.org/abs/2504.06279}, 
}

@misc{zhou2025surveyllmtimesdata,
      title={A Survey of LLM $\times$ DATA}, 
      author={Xuanhe Zhou and Junxuan He and Wei Zhou and Haodong Chen and Zirui Tang and Haoyu Zhao and Xin Tong and Guoliang Li and Youmin Chen and Jun Zhou and Zhaojun Sun and Binyuan Hui and Shuo Wang and Conghui He and Zhiyuan Liu and Jingren Zhou and Fan Wu},
      year={2025},
      eprint={2505.18458},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2505.18458}, 
}

@misc{zhou2024llmenhanceddatamanagement,
      title={LLM-Enhanced Data Management}, 
      author={Xuanhe Zhou and Xinyang Zhao and Guoliang Li},
      year={2024},
      eprint={2402.02643},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2402.02643}, 
}

@article{10.14778/3685800.3685838,
    author = {Li, Guoliang and Zhou, Xuanhe and Zhao, Xinyang},
    title = {LLM for Data Management},
    year = {2024},
    issue_date = {August 2024},
    publisher = {VLDB Endowment},
    volume = {17},
    number = {12},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/3685800.3685838},
    doi = {10.14778/3685800.3685838},
    abstract = {Machine learning techniques have been verified to be effective in optimizing data management systems and are widely researched in recent years. However, traditional small-sized ML models often struggle to generalize to new scenarios, and have limited context understanding ability (e.g., inputting discrete features only). The emergence of LLMs offers a promising solution to these challenges. LLMs have been trained over a vast number of scenarios and tasks and acquire human-competitive capabilities like context understanding and summarization, which can be highly beneficial for data management tasks (e.g., natural language based data analytics). In this tutorial, we present how to utilize LLMs to optimize data management systems and review new techniques for addressing these technical challenges, including hallucination of LLMs, high cost of interacting with LLMs, and low accuracy for processing complicated tasks. First, we discuss retrieval augmented generation (RAG) techniques to address the hallucination problem. Second, we present vector database techniques to improve the latency. Third, we present LLM agent techniques for processing complicated tasks by generating multi-round pipelines. We also showcase some real-world data management scenarios that can be well optimized by LLMs, including query rewrite, database diagnosis and data analytics. Finally, we summarize some open research challenges.},
    journal = {Proc. VLDB Endow.},
    month = aug,
    pages = {4213–4216},
    numpages = {4}
}

@inproceedings{10.1145/3613905.3651042,
    author = {Guo, Jiajing and Mohanty, Vikram and Piazentin Ono, Jorge H and Hao, Hongtao and Gou, Liang and Ren, Liu},
    title = {Investigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis},
    year = {2024},
    isbn = {9798400703317},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3613905.3651042},
    doi = {10.1145/3613905.3651042},
    abstract = {Despite demonstrating robust capabilities in performing tasks related to general-domain data-operation tasks, Large Language Models (LLMs) may exhibit shortcomings when applied to domain-specific tasks. We consider the design of domain-specific AI-powered data analysis tools from two dimensions: interaction and user agency. We implemented two design probes that fall on the two ends of the two dimensions: an open-ended high agency (OHA) prototype and a structured low agency (SLA) prototype. We conducted an interview study with nine data scientists to investigate (1) how users perceived the LLM outputs for data analysis assistance, and (2) how the two design probes, OHA and SLA, affected user behavior, performance, and perceptions. Our study revealed insights regarding participants’ interactions with LLMs, how they perceived the results, and their desire for explainability concerning LLM outputs, along with a noted need for collaboration with other users, and how they envisioned the utility of LLMs in their workflow.},
    booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
    articleno = {203},
    numpages = {9},
    keywords = {Large language model, data analysis, domain knowledge, human-AI collaboration, user agency},
    location = {Honolulu, HI, USA},
    series = {CHI EA '24}
}

@misc{rajkumar2022evaluatingtexttosqlcapabilitieslarge,
      title={Evaluating the Text-to-SQL Capabilities of Large Language Models}, 
      author={Nitarshan Rajkumar and Raymond Li and Dzmitry Bahdanau},
      year={2022},
      eprint={2204.00498},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.00498}, 
}

@inproceedings{sbbd,
 author = {Beatriz Miranda and Claudio E. C. Campelo},
 title = { How effective is an LLM-based Data Analysis Automation Tool? A Case Study with ChatGPT's Data Analyst},
 booktitle = {Anais do XXXIX Simpósio Brasileiro de Bancos de Dados},
 location = {Florianópolis/SC},
 year = {2024},
 keywords = {},
 issn = {2763-8979},
 pages = {287--299},
 publisher = {SBC},
 address = {Porto Alegre, RS, Brasil},
 doi = {10.5753/sbbd.2024.240841},
 url = {https://sol.sbc.org.br/index.php/sbbd/article/view/30700}
}

@article{10.1145/3737873,
    author = {Shi, Liang and Tang, Zhengju and Zhang, Nan and Zhang, Xiaotong and Yang, Zhi},
    title = {A Survey on Employing Large Language Models for Text-to-SQL Tasks},
    year = {2025},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3737873},
    doi = {10.1145/3737873},
    abstract = {With the development of the Large Language Models (LLMs), a large range of LLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a comprehensive review of LLM-based Text2SQL studies. We first enumerate classic benchmarks and evaluation metrics. For the two mainstream methods, prompt engineering and finetuning, we introduce a comprehensive taxonomy and offer practical insights into each subcategory. We present an overall analysis of the above methods and various models evaluated on well-known datasets and extract some characteristics. Finally, we discuss the challenges and future directions in this field.},
    note = {Just Accepted},
    journal = {ACM Comput. Surv.},
    month = jun,
    keywords = {Large Language Models, Text-to-SQL, Prompt Engineering, Fine-tuning}
}

@article{10.1371/journal.pone.0317084,
    doi = {10.1371/journal.pone.0317084},
    author = {Jansen, Jacqueline A. AND Manukyan, Artür AND Al Khoury, Nour AND Akalin, Altuna},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Leveraging large language models for data analysis automation},
    year = {2025},
    month = {02},
    volume = {20},
    url = {https://doi.org/10.1371/journal.pone.0317084},
    pages = {1-17},
    abstract = {Data analysis is constrained by a shortage of skilled experts, particularly in biology, where detailed data analysis and subsequent interpretation is vital for understanding complex biological processes and developing new treatments and diagnostics. One possible solution to this shortage in experts would be making use of Large Language Models (LLMs) for generating data analysis pipelines. However, although LLMs have shown great potential when used for code generation tasks, questions regarding the accuracy of LLMs when prompted with domain expert questions such as omics related data analysis questions, remain unanswered. To address this, we developed mergen, an R package that leverages LLMs for data analysis code generation and execution. We evaluated the performance of this data analysis system using various data analysis tasks for genomics. Our primary goal is to enable researchers to conduct data analysis by simply describing their objectives and the desired analyses for specific datasets through clear text. Our approach improves code generation via specialized prompt engineering and error feedback mechanisms. In addition, our system can execute the data analysis workflows prescribed by the LLM providing the results of the data analysis workflow for human review. Our evaluation of this system reveals that while LLMs effectively generate code for some data analysis tasks, challenges remain in executable code generation, especially for complex data analysis tasks. The best performance was seen with the self-correction mechanism, in which self-correct was able to increase the percentage of executable code when compared to the simple strategy by 22.5\% for tasks of complexity 2. For tasks for complexity 3, 4 and 5, this increase was 52.5\%, 27.5\% and 15\% respectively. Using a chi-squared test, it was shown that significant differences could be found using the different prompting strategies. Our study contributes to a better understanding of LLM capabilities and limitations, providing software infrastructure and practical insights for their effective integration into data analysis workflows.},
    number = {2},
}

@article{https://doi.org/10.1002/smr.2723,
    author = {Nejjar, Mohamed and Zacharias, Luca and Stiehle, Fabian and Weber, Ingo},
    title = {LLMs for science: Usage for code generation and data analysis},
    journal = {Journal of Software: Evolution and Process},
    volume = {37},
    number = {1},
    pages = {e2723},
    keywords = {artificial intelligence, code generation, data analysis, GenAI4Science, large language models, LLMs4Science, research methods},
    doi = {https://doi.org/10.1002/smr.2723},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2723},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2723},
    abstract = {Abstract Large language models (LLMs) have been touted to enable increased productivity in many areas of today's work life. Scientific research as an area of work is no exception: The potential of LLM-based tools to assist in the daily work of scientists has become a highly discussed topic across disciplines. However, we are only at the very onset of this subject of study. It is still unclear how the potential of LLMs will materialize in research practice. With this study, we give first empirical evidence on the use of LLMs in the research process. We have investigated a set of use cases for LLM-based tools in scientific research and conducted a first study to assess to which degree current tools are helpful. In this position paper, we report specifically on use cases related to software engineering, specifically, on generating application code and developing scripts for data analytics and visualization. While we studied seemingly simple use cases, results across tools differ significantly. Our results highlight the promise of LLM-based tools in general, yet we also observe various issues, particularly regarding the integrity of the output these tools provide.},
    year = {2025}
}

@misc{metallama3,
  author = {Meta},
  title = {Introducing Meta Llama 3: The most capable openly available LLM to date},
  publisher = {Meta},
  howpublished = "\url{https://ai.meta.com/blog/meta-llama-3/}",
  year = {},
}

@article{10.1145/3458754,
    author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
    title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
    year = {2021},
    issue_date = {January 2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {3},
    number = {1},
    url = {https://doi.org/10.1145/3458754},
    doi = {10.1145/3458754},
    abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at .},
    journal = {ACM Trans. Comput. Healthcare},
    month = oct,
    articleno = {2},
    numpages = {23},
    keywords = {domain-specific pretraining, NLP, Biomedical}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{joshi2017triviaqa,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1601--1611},
  year={2017}
}

@article{tsatsaronis2015overview,
  title={An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition},
  author={Tsatsaronis, George and Balikas, Georgios and Malakasiotis, Prodromos and Partalas, Ioannis and Zschunke, Matthias and Alvers, Michael R and Weissenborn, Dirk and Krithara, Anastasia and Petridis, Sergios and Polychronopoulos, Dimitris and others},
  journal={BMC bioinformatics},
  volume={16},
  pages={1--28},
  year={2015},
  publisher={Springer}
}


@inproceedings{mallen2023not,
  title={When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories},
  author={Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9802--9822},
  year={2023}
}

@misc{wang2025astuteragovercomingimperfect,
      title={Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models}, 
      author={Fei Wang and Xingchen Wan and Ruoxi Sun and Jiefeng Chen and Sercan Ö. Arık},
      year={2025},
      eprint={2410.07176},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.07176}, 
}

@inproceedings{10.1145/3701716.3715490,
    author = {Chan, Brian J. and Chen, Chao-Ting and Cheng, Jui-Hung and Huang, Hen-Hsen},
    title = {Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks},
    year = {2025},
    isbn = {9798400713316},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3701716.3715490},
    doi = {10.1145/3701716.3715490},
    abstract = {Retrieval-augmented generation (RAG) has gained traction as a powerful approach for enhancing language models by integrating external knowledge sources. However, RAG introduces challenges such as retrieval latency, potential errors in document selection, and increased system complexity. With the advent of large language models (LLMs) featuring significantly extended context windows, this paper proposes an alternative paradigm, cache-augmented generation (CAG) that bypasses real-time retrieval. Our method involves preloading all relevant resources, especially when the documents or knowledge for retrieval are of a limited and manageable size, into the LLM's extended context and caching its runtime parameters. During inference, the model utilizes these preloaded parameters to answer queries without additional retrieval steps. Comparative analyses reveal that CAG eliminates retrieval latency and minimizes retrieval errors while maintaining context relevance. Performance evaluations across multiple benchmarks highlight scenarios where long-context LLMs either outperform or complement traditional RAG pipelines. These findings suggest that, for certain applications, particularly those with a constrained knowledge base, CAG provide a streamlined and efficient alternative to RAG, achieving comparable or superior results with reduced complexity.},
    booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
    pages = {893–897},
    numpages = {5},
    location = {Sydney NSW, Australia},
    series = {WWW '25}
}

@article{10921633,
  author={Hindi, Mahd and Mohammed, Linda and Maaz, Ommama and Alwarafy, Abdulmalik},
  journal={IEEE Access}, 
  title={Enhancing the Precision and Interpretability of Retrieval-Augmented Generation (RAG) in Legal Technology: A Survey}, 
  year={2025},
  volume={13},
  number={},
  pages={46171-46189},
  keywords={Law;Retrieval augmented generation;Pipelines;Accuracy;Knowledge graphs;Surveys;Generators;Transformers;Large language models;Complexity theory;Information retrieval;large language model (LLM);legal technology;prompt engineering;retrieval-augmented generation (RAG)},
  doi={10.1109/ACCESS.2025.3550145}
}

@article{Chen_Lin_Han_Sun_2024, 
    title={Benchmarking Large Language Models in Retrieval-Augmented Generation}, 
    volume={38}, 
    url={https://ojs.aaai.org/index.php/AAAI/article/view/29728}, 
    DOI={10.1609/aaai.v38i16.29728}, 
    abstractNote={Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.}, 
    number={16}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le}, 
    year={2024}, 
    month=mar, 
    pages={17754-17762} 
}

@phdthesis{ammann2024analysis,
  title={Analysis of Risks and Mitigation Strategies in RAG},
  author={Ammann, Lukas and Ott, Sara},
  year={2024},
  month=dec,
  school={OST Ostschweizer Fachhochschule},
  note={Available at \url{https://eprints.ost.ch/id/eprint/1255/}},
}

@article{rane2023contribution,
  title={Contribution and performance of ChatGPT and other Large Language Models (LLM) for scientific and research advancements: a double-edged sword},
  author={Rane, Nitin Liladhar and Tawde, Abhijeet and Choudhary, Saurabh P and Rane, Jayesh},
  journal={International Research Journal of Modernization in Engineering Technology and Science},
  volume={5},
  number={10},
  pages={875--899},
  year={2023}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}

@misc{codestral,
  author = {MistralAI},
  title = {{Codestral}},
  publisher = {MistralAI},
  howpublished = "\url{https://mistral.ai/news/codestral/}",
  year = {2024}
}

@misc{largemistral,
  author = {MistralAI},
  title = {{Large Enough}},
  publisher = {MistralAI},
  howpublished = "\url{https://mistral.ai/news/mistral-large-2407}",
  year = {2024}
}

@misc{doosthosseini2024chataiseamlessslurmnative,
      title={Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services}, 
      author={Ali Doosthosseini and Jonathan Decker and Hendrik Nolte and Julian M. Kunkel},
      year={2024},
      eprint={2407.00110},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2407.00110}, 
}

@misc{wang2024improvingtextembeddingslarge,
      title={Improving Text Embeddings with Large Language Models}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Linjun Yang and Rangan Majumder and Furu Wei},
      year={2024},
      eprint={2401.00368},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.00368}, 
}

@misc{openai_like,
    author = {LlamaIndex},
    title = {OpenAILike},
    year = {},
    howpublished = {\url{https://docs.llamaindex.ai/en/stable/api_reference/llms/openai_like/}},
    note = {Online; accessed 2025-07-14}
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@mastersthesis{deshpande2025retrieval,
    title={Retrieval-Augmented Large Code Generation and Evaluation using Large Language Models},
    author={Deshpande, Bhushan},
    year={2025},
    school={Indian Institute of Science Education and Research Pune},
    note={Available at \url{http://dr.iiserpune.ac.in:8080/xmlui/handle/123456789/9958}},
}

@misc{arora2024optimizinglargelanguagemodel,
      title={Optimizing Large Language Model Hyperparameters for Code Generation}, 
      author={Chetan Arora and Ahnaf Ibn Sayeed and Sherlock Licorish and Fanyu Wang and Christoph Treude},
      year={2024},
      eprint={2408.10577},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2408.10577}, 
}

@article{10.1145/3697010,
    author = {Ouyang, Shuyin and Zhang, Jie M. and Harman, Mark and Wang, Meng},
    title = {An Empirical Study of the Non-Determinism of ChatGPT in Code Generation},
    year = {2025},
    issue_date = {February 2025},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {34},
    number = {2},
    issn = {1049-331X},
    url = {https://doi.org/10.1145/3697010},
    doi = {10.1145/3697010},
    abstract = {There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; non-deterministically returning very different code for the same prompt. Such non-determinism affects the correctness and consistency of the generated code, undermines developers’ trust in LLMs, and yields low reproducibility in LLM-based papers. Nevertheless, there is no work investigating how serious this non-determinism threat is.To fill this gap, this article conducts an empirical study on the non-determinism of ChatGPT in code generation. We chose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems across three code generation benchmarks (i.e., CodeContests, APPS and HumanEval) with three aspects of code similarities: semantic similarity, syntactic similarity, and structural similarity. Our results reveal that ChatGPT exhibits a high degree of non-determinism under the default setting: the ratio of coding tasks with zero equal test output across different requests is 75.76\%, 51.00\% and 47.56\% for three different code generation datasets (i.e., CodeContests, APPS and HumanEval), respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration (temperature  (=)  1). In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.},
    journal = {ACM Trans. Softw. Eng. Methodol.},
    month = jan,
    articleno = {42},
    numpages = {28},
    keywords = {code generation, non-determinism, large language model}
}

@misc{gpt-4o,
  author = {OpenAI},
  title = {{Hello GPT-4o}},
  publisher = {OpenAI},
  howpublished = "\url{https://openai.com/index/hello-gpt-4o/}",
  year = {2024}
}

@misc{deepseekai2025deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
      year={2025},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@misc{gao2025comparisondeepseekllms,
      title={A Comparison of DeepSeek and Other LLMs}, 
      author={Tianchen Gao and Jiashun Jin and Zheng Tracy Ke and Gabriel Moryoussef},
      year={2025},
      eprint={2502.03688},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.03688}, 
}

@article{puspitasari2025deepseek,
  title={Deepseek models: A comprehensive survey of methods and applications},
  author={Puspitasari, Fachrina Dewi and Zhang, Chaoning and Dam, Sumit Kumar and Zhang, Mengchun and Kim, Tae-Ho and Hong, Choong Seon and Bae, Sung-Ho and Qin, Caiyan and Wei, Jiwei and Wang, Guoqing and others},
  journal={Authorea Preprints},
  year={2025},
  month=mar,
  publisher={Authorea},
  doi={10.36227/techrxiv.174198511.15158242/v1}
}

@article{vujovic2021classification,
  title={Classification model evaluation metrics},
  author={Vujovi{\'c}, {\v{Z}}eljko and others},
  journal={International Journal of Advanced Computer Science and Applications},
  volume={12},
  number={6},
  pages={599--606},
  year={2021}
}

@misc{radon,
    author = {Radon},
    title = {Radon documentation},
    url = "https://radon.readthedocs.io/en/latest/index.html",
    month = {},
    year = {},
    note = "[Online; accessed 2025-07-23]"
}

@misc{farchi2024automaticgenerationbenchmarksreliable,
      title={Automatic Generation of Benchmarks and Reliable LLM Judgment for Code Tasks}, 
      author={Eitan Farchi and Shmulik Froimovich and Rami Katan and Orna Raz},
      year={2024},
      eprint={2410.21071},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2410.21071}, 
}

@inproceedings{10.1145/3338906.3342494,
    author = {He, Hao},
    title = {Understanding source code comments at large-scale},
    year = {2019},
    isbn = {9781450355728},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3338906.3342494},
    doi = {10.1145/3338906.3342494},
    abstract = {Source code comments are important for any software, but the basic patterns of writing comments across domains and programming languages remain unclear. In this paper, we take a first step toward understanding differences in commenting practices by analyzing the comment density of 150 projects in 5 different programming languages. We have found that there are noticeable differences in comment density, which may be related to the programming language used in the project and the purpose of the project.},
    booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
    pages = {1217–1219},
    numpages = {3},
    keywords = {Source Code Comments, Empirical Study, Comment Density},
    location = {Tallinn, Estonia},
    series = {ESEC/FSE 2019}
}

@misc{licorish2025comparinghumanllmgenerated,
      title={Comparing Human and LLM Generated Code: The Jury is Still Out!}, 
      author={Sherlock A. Licorish and Ansh Bajpai and Chetan Arora and Fanyu Wang and Kla Tantithamthavorn},
      year={2025},
      eprint={2501.16857},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2501.16857}, 
}

@misc{takerngsaksiri2025codereadabilityagelarge,
      title={Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian}, 
      author={Wannita Takerngsaksiri and Chakkrit Tantithamthavorn and Micheal Fu and Jirat Pasuksmit and Kun Chen and Ming Wu},
      year={2025},
      eprint={2501.11264},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2501.11264}, 
}

@ARTICLE{10507163,
  author={Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng},
  journal={IEEE Transactions on Software Engineering}, 
  title={No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT}, 
  year={2024},
  volume={50},
  number={6},
  pages={1548-1584},
  keywords={Codes;Chatbots;Task analysis;Complexity theory;Security;Transformers;Electronic mail;Large language model;ChatGPT;code generation},
  doi={10.1109/TSE.2024.3392499}
}

@article{bucur2006quality,
  title={On Quality and Measures in Software Engineering.},
  author={Bucur, Ion I},
  journal={Journal of Applied Quantitative Methods},
  volume={1},
  number={2},
  pages={210--217},
  year={2006},
  publisher={ERIC}
}

@article{khan_evaluating_2023,
	title = {Evaluating the effectiveness of decomposed {Halstead} {Metrics} in software fault prediction},
	volume = {9},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-1647},
	doi = {10.7717/peerj-cs.1647},
	abstract = {The occurrence of faults in software systems represents an inevitable predicament. Testing is the most common means to detect such faults; however, exhaustive testing is not feasible for any nontrivial system. Software fault prediction (SFP), which identifies software components that are more prone to errors, seeks to supplement the testing process. Thus, testing efforts can be focused on such modules. Various approaches exist for SFP, with machine learning (ML) emerging as the prevailing methodology. ML-based SFP relies on a wide range of metrics, ranging from file-level and class-level to method-level and even line-level metrics. More granularized metrics are expected to possess a higher degree of micro-level coverage of the code. The Halstead metric suite offers coverage at the line level and has been extensively employed across diverse domains such as fault prediction, quality assessment, and similarity approximation for the past three decades. In this article, we propose to decompose Halstead base metrics and evaluate their fault prediction capability. The Halstead base metrics consist of operators and operands. In the context of the Java language, we partition operators into five distinct categories, i.e., assignment operators, arithmetic operators, logical operators, relational operators, and all other types of operators. Similarly, operands are classified into two classes: constants and variables. For the purpose of empirical evaluation, two experiments were designed. In the first experiment, the Halstead base metrics were used along with McCabe, Lines of Code (LoC), and Halstead-derived metrics as predictors. In the second experiment, decomposed Halstead base metrics were used along with McCabe, LoC, and Halstead-derived metrics. Five public datasets were selected for the experiments. The ML classifiers used included logistic regression, naïve Bayes, decision tree, multilayer perceptron, random forest, and support vector machines. The ML classifiers’ effectiveness was assessed through metrics such as accuracy, F-measure, and AUC. Accuracy saw an enhancement from 0.82 to 0.97, while F-measure exhibited improvement from 0.81 to 0.99. Correspondingly, the AUC value advanced from 0.79 to 0.99. These findings highlight the superior performance of decomposed Halstead metrics, as opposed to the original Halstead base metrics, in predicting faults across all datasets.},
	language = {en},
	urldate = {2025-07-23},
	journal = {PeerJ Computer Science},
	author = {Khan, Bilal and Nadeem, Aamer},
	month = nov,
	year = {2023},
	note = {Publisher: PeerJ},
	pages = {e1647},
}

@article{welker2001software,
  title={The software maintainability index revisited},
  author={Welker, Kurt D},
  journal={CrossTalk},
  volume={14},
  pages={18--21},
  year={2001}
}

@INPROCEEDINGS{7302435,
  author={Counsell, S. and Liu, X. and Eldh, S. and Tonelli, R. and Marchesi, M. and Concas, G. and Murgia, A.},
  booktitle={2015 41st Euromicro Conference on Software Engineering and Advanced Applications}, 
  title={Re-visiting the 'Maintainability Index' Metric from an Object-Oriented Perspective}, 
  year={2015},
  volume={},
  number={},
  pages={84-87},
  keywords={Measurement;Correlation;Couplings;Complexity theory;Correlation coefficient;Maintenance engineering;Software;Metrics;Eclipse;empirical;maintainability;object-oriented},
  doi={10.1109/SEAA.2015.41}
}

@misc{gilboy_maintainability_2022,
	title = {Maintainability {Index} - {What} is it and where does it fall short?},
	url = {https://sourcery.ai/blog/maintainability-index},
	abstract = {Calculating the Maintainability Index and looking at how you can use it for your projects.},
	language = {en},
	urldate = {2025-07-23},
	author = {Gilboy, Tim},
	month = mar,
	year = {2022},
}

@book{10.5555/540137,
    author = {Halstead, Maurice H.},
    title = {Elements of Software Science (Operating and programming systems series)},
    year = {1977},
    isbn = {0444002057},
    publisher = {Elsevier Science Inc.},
    address = {USA}
}

@ARTICLE{1702388,
  author={McCabe, T.J.},
  journal={IEEE Transactions on Software Engineering}, 
  title={A Complexity Measure}, 
  year={1976},
  volume={SE-2},
  number={4},
  pages={308-320},
  keywords={Software testing;System testing;Graph theory;Fluid flow measurement;Software measurement;Linear programming;Software engineering;Software systems;Software maintenance;National security;Basis;complexity measure;control flow;decomposition;graph theory;independence;linear;modularization;programming;reduction;software;testing},
  doi={10.1109/TSE.1976.233837}
}

@INPROCEEDINGS {242525,
    author = { Oman, P. and Hagemeister, J. },
    booktitle = { Proceedings Conference on Software Maintenance 1992 },
    title = {{ Metrics for assessing a software system's maintainability }},
    year = {1992},
    volume = {},
    ISSN = {},
    pages = {337,338,339,340,341,342,343,344},
    abstract = { It is noted that the factors of software that determine or influence maintainability can be organized into a hierarchical structure of measurable attributes. For each of these attributes the authors show a metric definition consistent with the published definitions of the software characteristic being measured. The result is a tree structure of maintainability metrics which can be used for purposes of evaluating the relative maintainability of the software system. The authors define metrics for measuring the maintainability of a target software system and discuss how those metrics can be combined into a single index of maintainability. },
    keywords = {Software maintenance;Software systems;Software measurement;Tree data structures;Documentation;Taxonomy;Environmental management;Lifting equipment;Software engineering;Software testing},
    doi = {10.1109/ICSM.1992.242525},
    url = {https://doi.ieeecomputersociety.org/10.1109/ICSM.1992.242525},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month =Nov
}
